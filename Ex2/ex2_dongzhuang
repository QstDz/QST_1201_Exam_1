package exam;

import java.io.IOException;
import java.util.Comparator;
import java.util.HashSet;
import java.util.Set;
import java.util.TreeMap;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class ipquchong {

	public static class Map extends Mapper<LongWritable, Text, Text,IntWritable>{
		private HashSet<String>set=new HashSet<>();
		@Override
		protected void map(LongWritable key, Text value,
				Context context)
				throws IOException, InterruptedException {			
			String line = value.toString();
			String ip = line.split("\t")[0];
			set.add(ip);
			
		}
		@Override
		protected void cleanup(
				Context context)
				throws IOException, InterruptedException {
			//因为只需要去重，所以map处理一遍取到ip存到hashset中去重，直接输出hashset的长度就好了，不需要reduce了。
			//需要注意的是要在cleanup中做输出，因为cleanup是在map任务跑完之后才做的，否则会丢失数据
			
			context.write(new Text("ip's counters:"), new IntWritable(set.size()));
		}
		
	}

	
	public static void main(String[] args) throws IOException {
		Configuration conf =new Configuration();
		Job job=new Job(conf);
		job.setJobName("ip");
		job.setJarByClass(ipquchong.class);
		job.setMapperClass(Map.class);		
		job.setNumReduceTasks(0);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);		
		try {
			System.exit(job.waitForCompletion(true)?0:1);
		} catch (ClassNotFoundException | InterruptedException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}	
}
